{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CleanRL Overview CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: Single-file Implementation Every detail about an algorithm is put into the algorithm's own file. Therefore, it's easier for you to fully understand an algorithm and do research with it. Benchmarked Implementation on 7+ algorithms and 34+ games Tensorboard Logging Local Reproducibility via Seeding Videos of Gameplay Capturing Experiment Management with Weights and Biases Cloud Integration with Docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \ud83d\ude80 Citing CleanRL If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Overview"},{"location":"#cleanrl","text":"","title":"CleanRL"},{"location":"#overview","text":"CleanRL is a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features. The implementation is clean and simple, yet we can scale it to run thousands of experiments using AWS Batch. The highlight features of CleanRL are: Single-file Implementation Every detail about an algorithm is put into the algorithm's own file. Therefore, it's easier for you to fully understand an algorithm and do research with it. Benchmarked Implementation on 7+ algorithms and 34+ games Tensorboard Logging Local Reproducibility via Seeding Videos of Gameplay Capturing Experiment Management with Weights and Biases Cloud Integration with Docker and AWS You can read more about CleanRL in our technical paper and documentation . Good luck have fun \ud83d\ude80","title":"Overview"},{"location":"#citing-cleanrl","text":"If you use CleanRL in your work, please cite our technical paper : @article { huang2021cleanrl , title = {CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms} , author = {Shengyi Huang and Rousslan Fernand Julien Dossa and Chang Ye and Jeff Braga} , year = {2021} , eprint = {2111.08819} , archivePrefix = {arXiv} , primaryClass = {cs.LG} }","title":"Citing CleanRL"},{"location":"community/","text":"We have a Discord Community for support. Feel free to ask questions. Posting in Github Issues and PRs are also welcome. Also our past video recordings are available at YouTube Related Resources Deep Reinforcement Learning With TensorFlow 2.1 minimalRL - PyTorch Deep-Reinforcement-Learning-Hands-On Stable Baselines3 PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL). Reinforcement-Implementation","title":"Community"},{"location":"community/#related-resources","text":"Deep Reinforcement Learning With TensorFlow 2.1 minimalRL - PyTorch Deep-Reinforcement-Learning-Hands-On Stable Baselines3 PyTorch implementation of Advantage Actor Critic (A2C), Proximal Policy Optimization (PPO), Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation (ACKTR) and Generative Adversarial Imitation Learning (GAIL). Reinforcement-Implementation","title":"Related Resources"},{"location":"contribution/","text":"Thank you for being interested in contributing to our project. All kinds of contributions are welcome. Below are some steps to help you get started: Join our discord channel to say hi!! \ud83d\udc4b Pick something you want to work at and let us know on slack. You could Tackle issues with the help wanted flag Bug fixes and various improvements on existing algorithms Contribute to the Open RL Benchmark You could add new algorithms or new games to be featured in the Open RL Benchmark Free free to contact me (Costa) directly on slack. I will add you to the CleanRL's Team at Weight and Biases. Your experiments can be featured on the Open RL Benchmark . Submit a PR and get it merged! \ud83c\udf87 Good luck and have fun!","title":"Contribution"},{"location":"made-with-cleanrl/","text":"Made with CleanRL CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL. Publications An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., Preprint. Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#made-with-cleanrl","text":"CleanRL has become an increasingly popular deep reinforcement learning library, especially among practitioners who prefer more customizable code. Since its debut in July 2019, CleanRL has supported many open source projects and publications. Below are some highlight projects and publications made with CleanRL. Feel free to edit this list if your project or paper has used CleanRL.","title":"Made with CleanRL"},{"location":"made-with-cleanrl/#publications","text":"An Empirical Investigation of Early Stopping Optimizations in Proximal Policy Optimization , Dossa, R., Huang, S., Onta\u00f1\u00f3n, S., Matsubara, T., IEEE Access, 2021 Gym-\u03bcRTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning , Huang, S., Onta\u00f1\u00f3n, S., Bamford, C., Grela, L., IEEE Conference on Games 2021 Measuring Generalization of Deep Reinforcement Learning Applied to Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AAAI 2021 Reinforcement Learning in Games Workshop Griddly: A platform for AI research in games , Bamford, C., Huang, S., Lucas, S., AAAI 2021 Reinforcement Learning in Games Workshop Action Guidance: Getting the Best of Sparse Rewards and Shaped Rewards for Real-time Strategy Games , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2020 A Closer Look at Invalid Action Masking in Policy Gradient Algorithms , Huang, S., Onta\u00f1\u00f3n, S., Preprint. Comparing Observation and Action Representations for Reinforcement Learning in \u00b5RTS , Huang, S., Onta\u00f1\u00f3n, S., AIIDE Workshop on Artificial Intelligence for Strategy Games , October 2019","title":"Publications"},{"location":"open-rl-benchmark/","text":"","title":"Open RL Benchmark"},{"location":"rl-algorithms/","text":"Below are the implemented algorithms and their brief descriptions. [x] Deep Q-Learning (DQN) dqn.py For discrete action space. dqn_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. [x] Categorical DQN (C51) c51.py For discrete action space. c51_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. c51_atari_visual.py Adds return and q-values visulization for dqn_atari.py . [x] Proximal Policy Gradient (PPO) All of the PPO implementations below are augmented with some code-level optimizations. See https://costa.sh/blog-the-32-implementation-details-of-ppo.html for more details ppo.py For discrete action space. ppo_continuous_action.py For continuous action space. Also implemented Mujoco-specific code-level optimizations ppo_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. [x] Soft Actor Critic (SAC) sac_continuous_action.py For continuous action space. [x] Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py For continuous action space. [x] Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py For continuous action space. [x] Apex Deep Q-Learning (Apex-DQN) apex_dqn_atari_visual.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques.","title":"Rl algorithms"},{"location":"advanced/resume-training/","text":"Resume Training A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question. Save model checkpoints The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows Resume training The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume Training"},{"location":"advanced/resume-training/#resume-training","text":"A common question we get asked is how to set up model checkpoints to continue training. In this document, we take this PPO example to explain that question.","title":"Resume Training"},{"location":"advanced/resume-training/#save-model-checkpoints","text":"The first step is to save models periodically. By default, we save the model to wandb . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) Then we could run the following to train our agents python ppo_gridnet.py --prod-mode --capture-video If the training was terminated early, we can still see the last updated model agent.pt in W&B like in this URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda/files or as follows","title":"Save model checkpoints"},{"location":"advanced/resume-training/#resume-training_1","text":"The second step is to automatically download the agent.pt from the URL above and resume training as follows: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 num_updates = args . total_timesteps // args . batch_size CHECKPOINT_FREQUENCY = 50 starting_update = 1 if args . track and wandb . run . resumed : starting_update = run . summary . get ( \"charts/update\" ) + 1 global_step = starting_update * args . batch_size api = wandb . Api () run = api . run ( f \" { run . entity } / { run . project } / { run . id } \" ) model = run . file ( \"agent.pt\" ) model . download ( f \"models/ { experiment_name } /\" ) agent . load_state_dict ( torch . load ( f \"models/ { experiment_name } /agent.pt\" , map_location = device )) agent . eval () print ( f \"resumed at update { starting_update } \" ) for update in range ( starting_update , num_updates + 1 ): # ... do rollouts and train models if args . track : # make sure to tune `CHECKPOINT_FREQUENCY` # so models are not saved too frequently if update % CHECKPOINT_FREQUENCY == 0 : torch . save ( agent . state_dict (), f \" { wandb . run . dir } /agent.pt\" ) wandb . save ( f \" { wandb . run . dir } /agent.pt\" , policy = \"now\" ) To resume training, note the ID of the experiment is 21421tda as in the URL https://wandb.ai/costa-huang/cleanRL/runs/21421tda , so we need to pass in the ID via environment variable to trigger the resume mode of W&B: WANDB_RUN_ID=21421tda WANDB_RESUME=must python ppo_gridnet.py --prod-mode --capture-video","title":"Resume training"},{"location":"cloud/installation/","text":"Installation The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently. Prerequisites Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like Clean Up Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Installation"},{"location":"cloud/installation/#installation","text":"The rough idea behind the cloud integration is to package our code into a docker container and use AWS Batch to run thousands of experiments concurrently.","title":"Installation"},{"location":"cloud/installation/#prerequisites","text":"Terraform (see installation tutorial here ) We use Terraform to define our infrastructure with AWS Batch, which you can spin up as follows # assuming you are at the root of the CleanRL project poetry install -E cloud cd cloud python -m awscli configure terraform init export AWS_DEFAULT_REGION = $( aws configure get region --profile default ) terraform apply Note Don't worry about the cost of spining up these AWS Batch compute environments and job queues. They are completely free and you are only charged when you submit experiments. Then your AWS Batch console should look like","title":"Prerequisites"},{"location":"cloud/installation/#clean-up","text":"Uninstalling/Deleting the infrastructure is pretty straightforward: export AWS_DEFAULT_REGION=$(aws configure get region --profile default) terraform destroy","title":"Clean Up"},{"location":"cloud/submit-experiments/","text":"Submit Experiments Inspection Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\" Run on AWS Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see: Customize the Docker Container Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#submit-experiments","text":"","title":"Submit Experiments"},{"location":"cloud/submit-experiments/#inspection","text":"Dry run to inspect the generated docker command poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --num-seed 1 The generated docker command should look like docker run -d --cpuset-cpus=\"0\" -e WANDB_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx vwxyzjn/cleanrl:latest /bin/bash -c \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video --seed 1\"","title":"Inspection"},{"location":"cloud/submit-experiments/#run-on-aws","text":"Submit a job using AWS's compute-optimized spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing spot instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge-spot \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's compute-optimized on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --job-queue c5a-large \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-memory 2000 \\ --num-hours 48.0 \\ --provider aws Submit a job using AWS's accelerated-computing on-demand instances poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 --track --capture-video\" \\ --job-queue g4dn-xlarge \\ --num-seed 1 \\ --num-vcpu 1 \\ --num-gpu 1 \\ --num-memory 4000 \\ --num-hours 48.0 \\ --provider aws Then you should see:","title":"Run on AWS"},{"location":"cloud/submit-experiments/#customize-the-docker-container","text":"Set up docker's buildx and login in to your preferred registry. docker buildx create --use docker login Then you could build a container using the --build flag based on the Dockerfile in the current directory. Also, --push will auto-push to the docker registry. poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --build --push To build a multi-arch image using --archs linux/arm64,linux/amd64 : poetry run python -m cleanrl_utils.submit_exp \\ --docker-tag vwxyzjn/cleanrl:latest \\ --command \"poetry run python cleanrl/ppo.py --env-id CartPole-v1 --total-timesteps 100000 --track --capture-video\" \\ --archs linux/arm64,linux/amd64 --build --push Note Building an multi-arch image is quite slow but will allow you to use ARM instances such as m6gd.medium that is 20-70% cheaper than X86 instances. However, note there is no cloud providers that give ARM instances with Nvidia's GPU (to my knowledge), so this effort might not be worth it. If you still wants to pursue multi-arch, you can speed things up by using a native ARM server and connect it to your buildx instance: docker -H ssh://costa@gpu info docker buildx create --name remote --use docker buildx create --name remote --append ssh://costa@gpu docker buildx inspect --bootstrap python -m cleanrl_utils.submit_exp -b --archs linux/arm64,linux/amd64","title":"Customize the Docker Container"},{"location":"get-started/basic-usage/","text":"Basic Usage Two Ways to Run After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell. Visualize Training Metrics By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs Visualize the Agent's Gameplay Videos CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video Get Documentation You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Basic Usage"},{"location":"get-started/basic-usage/#basic-usage","text":"","title":"Basic Usage"},{"location":"get-started/basic-usage/#two-ways-to-run","text":"After the dependencies have been installed, there are two ways to run the CleanRL script under the poetry virtual environments. Using poetry run : poetry run python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Using poetry shell : We first activate the virtual environment by using poetry shell Then, run any desired CleanRL script Attention: Each step must be executed separately! poetry shell python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 Note We recommend poetry shell workflow for development. When the shell is activeated, you should be seeing a prefix like (cleanrl-iXg02GqF-py3.9) in your shell's prompt, which is the name of the poetry's virtual environment. We will assume to run other commands (e.g. tensorboard ) in the documentation within the poetry's shell.","title":"Two Ways to Run"},{"location":"get-started/basic-usage/#visualize-training-metrics","text":"By default, the CleanRL scripts record all the training metrics via Tensorboard into the runs folder. So, after running the training script above, feel free to run tensorboard --logdir runs","title":"Visualize Training Metrics"},{"location":"get-started/basic-usage/#visualize-the-agents-gameplay-videos","text":"CleanRL helps record the agent's gameplay videos with a --capture-video flag, which will save the videos in the videos/{$run_name} folder. 1 2 3 4 5 python cleanrl/ppo.py \\ --seed 1 \\ --env-id CartPole-v0 \\ --total-timesteps 50000 \\ --capture-video","title":"Visualize the Agent's Gameplay Videos"},{"location":"get-started/basic-usage/#get-documentation","text":"You can directly obtained the documentation by using the --help flag. python cleanrl/ppo.py --help usage: ppo.py [ -h ] [ --exp-name EXP_NAME ] [ --env-id ENV_ID ] [ --learning-rate LEARNING_RATE ] [ --seed SEED ] [ --total-timesteps TOTAL_TIMESTEPS ] [ --torch-deterministic [ TORCH_DETERMINISTIC ]] [ --cuda [ CUDA ]] [ --track [ TRACK ]] [ --wandb-project-name WANDB_PROJECT_NAME ] [ --wandb-entity WANDB_ENTITY ] [ --capture-video [ CAPTURE_VIDEO ]] [ --num-envs NUM_ENVS ] [ --num-steps NUM_STEPS ] [ --anneal-lr [ ANNEAL_LR ]] [ --gae [ GAE ]] [ --gamma GAMMA ] [ --gae-lambda GAE_LAMBDA ] [ --num-minibatches NUM_MINIBATCHES ] [ --update-epochs UPDATE_EPOCHS ] [ --norm-adv [ NORM_ADV ]] [ --clip-coef CLIP_COEF ] [ --clip-vloss [ CLIP_VLOSS ]] [ --ent-coef ENT_COEF ] [ --vf-coef VF_COEF ] [ --max-grad-norm MAX_GRAD_NORM ] [ --target-kl TARGET_KL ] optional arguments: -h, --help show this help message and exit --exp-name EXP_NAME the name of this experiment --env-id ENV_ID the id of the environment --learning-rate LEARNING_RATE the learning rate of the optimizer --seed SEED seed of the experiment --total-timesteps TOTAL_TIMESTEPS total timesteps of the experiments --torch-deterministic [ TORCH_DETERMINISTIC ] if toggled, ` torch.backends.cudnn.deterministic = False ` --cuda [ CUDA ] if toggled, cuda will be enabled by default --track [ TRACK ] if toggled, this experiment will be tracked with Weights and Biases --wandb-project-name WANDB_PROJECT_NAME the wandb 's project name --wandb-entity WANDB_ENTITY the entity (team) of wandb' s project --capture-video [ CAPTURE_VIDEO ] weather to capture videos of the agent performances ( check out ` videos ` folder ) --num-envs NUM_ENVS the number of parallel game environments --num-steps NUM_STEPS the number of steps to run in each environment per policy rollout --anneal-lr [ ANNEAL_LR ] Toggle learning rate annealing for policy and value networks --gae [ GAE ] Use GAE for advantage computation --gamma GAMMA the discount factor gamma --gae-lambda GAE_LAMBDA the lambda for the general advantage estimation --num-minibatches NUM_MINIBATCHES the number of mini-batches --update-epochs UPDATE_EPOCHS the K epochs to update the policy --norm-adv [ NORM_ADV ] Toggles advantages normalization --clip-coef CLIP_COEF the surrogate clipping coefficient --clip-vloss [ CLIP_VLOSS ] Toggles whether or not to use a clipped loss for the value function , as per the paper. --ent-coef ENT_COEF coefficient of the entropy --vf-coef VF_COEF coefficient of the value function --max-grad-norm MAX_GRAD_NORM the maximum norm for the gradient clipping --target-kl TARGET_KL the target KL divergence threshold","title":"Get Documentation"},{"location":"get-started/examples/","text":"Examples Atari poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/apex_dqn_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3 Demo You can also run training scripts in other games, such as: Classic Control poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1 PyBullet poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 Procgen poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot PPO + LSTM poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"Examples"},{"location":"get-started/examples/#examples","text":"","title":"Examples"},{"location":"get-started/examples/#atari","text":"poetry shell poetry install -E atari python cleanrl/dqn_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/c51_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_atari.py --env-id BreakoutNoFrameskip-v4 python cleanrl/apex_dqn_atari.py --env-id BreakoutNoFrameskip-v4 # NEW: 3-4x side-effects free speed up with envpool's atari (only available to linux) poetry install -E envpool python cleanrl/ppo_atari_envpool.py --env-id BreakoutNoFrameskip-v4 # Learn Pong-v5 in ~5-10 mins # Side effects such as lower sample efficiency might occur poetry run python ppo_atari_envpool.py --clip-coef=0.2 --num-envs=16 --num-minibatches=8 --num-steps=128 --update-epochs=3","title":"Atari"},{"location":"get-started/examples/#demo","text":"You can also run training scripts in other games, such as:","title":"Demo"},{"location":"get-started/examples/#classic-control","text":"poetry shell python cleanrl/dqn.py --env-id CartPole-v1 python cleanrl/ppo.py --env-id CartPole-v1 python cleanrl/c51.py --env-id CartPole-v1","title":"Classic Control"},{"location":"get-started/examples/#pybullet","text":"poetry shell poetry install -E pybullet python cleanrl/td3_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/ddpg_continuous_action.py --env-id MinitaurBulletDuckEnv-v0 python cleanrl/sac_continuous_action.py --env-id MinitaurBulletDuckEnv-v0","title":"PyBullet"},{"location":"get-started/examples/#procgen","text":"poetry shell poetry install -E procgen python cleanrl/ppo_procgen.py --env-id starpilot python cleanrl/ppg_procgen.py --env-id starpilot","title":"Procgen"},{"location":"get-started/examples/#ppo-lstm","text":"poetry shell poetry install -E atari python cleanrl/ppo_atari_lstm.py --env-id BreakoutNoFrameskip-v4 python cleanrl/ppo_memory_env_lstm.py","title":"PPO + LSTM"},{"location":"get-started/experiment-tracking/","text":"Experiment tracking To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/experiment-tracking/#experiment-tracking","text":"To use experiment tracking with wandb, run with the --track flag, which will also upload the videos recorded by the --capture-video flag. poetry shell wandb login # only required for the first time python cleanrl/ppo.py --track --capture-video The console will output the url for the tracked experiment like the following wandb: View project at https://wandb.ai/costa-huang/cleanRL wandb: View run at https://wandb.ai/costa-huang/cleanRL/runs/10dwbgeh When you open the URL, it's going to look like the following page:","title":"Experiment tracking"},{"location":"get-started/installation/","text":"Installation Prerequisites Python 3.8+ Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install Optional Dependencies CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E procgen poetry install -E pettingzoo poetry install -E plot poetry install -E cloud poetry install -E docs poetry install -E spyder","title":"Installation"},{"location":"get-started/installation/#installation","text":"","title":"Installation"},{"location":"get-started/installation/#prerequisites","text":"Python 3.8+ Poetry Simply run the following command for a quick start git clone https://github.com/vwxyzjn/cleanrl.git && cd cleanrl poetry install","title":"Prerequisites"},{"location":"get-started/installation/#optional-dependencies","text":"CleanRL makes it easy to install optional dependencies for common RL environments and various development utilities. These optional dependencies are defined at pyproject.toml as shown below: atari = [ \"ale-py\" , \"AutoROM\" , \"stable-baselines3\" ] pybullet = [ \"pybullet\" ] procgen = [ \"procgen\" , \"stable-baselines3\" ] pettingzoo = [ \"pettingzoo\" , \"stable-baselines3\" , \"pygame\" , \"pymunk\" ] plot = [ \"pandas\" , \"seaborn\" ] cloud = [ \"boto3\" , \"awscli\" ] docs = [ \"mkdocs-material\" ] spyder = [ \"spyder\" ] You can install them using the following command poetry install -E atari poetry install -E pybullet poetry install -E procgen poetry install -E pettingzoo poetry install -E plot poetry install -E cloud poetry install -E docs poetry install -E spyder","title":"Optional Dependencies"},{"location":"rl-algorithms/dqn/","text":"Deep Q-Learning (DQN) As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Playing Atari with Deep Reinforcement Learning Human-level control through deep reinforcement learning Our single-file implementations of DQN: dqn.py Works with the Box observation space of low-level features Works with the Discerete action space Works with envs like CartPole-v1 dqn_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discerete action space","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/dqn/#deep-q-learning-dqn","text":"As an extension of the Q-learning, DQN's main technical contribution is the use of replay buffer and target network, both of which would help improve the stability of the algorithm. Original papers: Playing Atari with Deep Reinforcement Learning Human-level control through deep reinforcement learning Our single-file implementations of DQN: dqn.py Works with the Box observation space of low-level features Works with the Discerete action space Works with envs like CartPole-v1 dqn_atari.py For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discerete action space","title":"Deep Q-Learning (DQN)"},{"location":"rl-algorithms/overview/","text":"Overview Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py ppo_procgen.py \u2705 Deep Q-Learning (DQN) dqn.py dqn_atari.py \u2705 Categorical DQN (C51) c51.py c51_atari.py \u2705 Apex Deep Q-Learning (Apex-DQN) apex_dqn_atari.py \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py","title":"Overview"},{"location":"rl-algorithms/overview/#overview","text":"Algorithm Variants Implemented \u2705 Proximal Policy Gradient (PPO) ppo.py , docs ppo_atari.py , docs ppo_continuous_action.py , docs ppo_atari_lstm.py ppo_procgen.py \u2705 Deep Q-Learning (DQN) dqn.py dqn_atari.py \u2705 Categorical DQN (C51) c51.py c51_atari.py \u2705 Apex Deep Q-Learning (Apex-DQN) apex_dqn_atari.py \u2705 Soft Actor-Critic (SAC) sac_continuous_action.py \u2705 Deep Deterministic Policy Gradient (DDPG) ddpg_continuous_action.py \u2705 Twin Delayed Deep Deterministic Policy Gradient (TD3) td3_continuous_action.py","title":"Overview"},{"location":"rl-algorithms/ppo/","text":"Proximal Policy Gradient (PPO) Overview PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . See The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm for more details. Implemented Variants Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations Below are our single-file implementations of PPO: ppo.py The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discerete action space Works with envs like CartPole-v1 Usage poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1 Implementation details ppo.py includes the 11 core implementation details: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 ) Experiment results PR vwxyzjn/cleanrl#120 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ppo . Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO CartPole-v1 488.75 \u00b1 18.40 497.54 \u00b1 4.02 Acrobot-v1 -82.48 \u00b1 5.93 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos: Video tutorial If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial: ppo_atari.py The ppo_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discerete action space Includes the 9 Atari-specific implementation details as shown in the following video tutorial ppo_continuous_action.py The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space Includes the 8 implementation details for as shown in the following video tutorial (need fixing)","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#proximal-policy-gradient-ppo","text":"","title":"Proximal Policy Gradient (PPO)"},{"location":"rl-algorithms/ppo/#overview","text":"PPO is one of the most popular DRL algorithms. It runs reasonably fast by leveraging vector (parallel) environments and naturally works well with different action spaces, therefore supporting a variety of games. It also has good sample efficiency compared to algorithms such as DQN. Original paper: Proximal Policy Optimization Algorithms Reference resources: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study All our PPO implementations below are augmented with the same code-level optimizations presented in openai/baselines 's PPO . See The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm for more details.","title":"Overview"},{"location":"rl-algorithms/ppo/#implemented-variants","text":"Variants Implemented Description ppo.py , docs For classic control tasks like CartPole-v1 . ppo_atari.py , docs For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. ppo_continuous_action.py , docs For continuous action space. Also implemented Mujoco-specific code-level optimizations Below are our single-file implementations of PPO:","title":"Implemented Variants"},{"location":"rl-algorithms/ppo/#ppopy","text":"The ppo.py has the following features: Works with the Box observation space of low-level features Works with the Discerete action space Works with envs like CartPole-v1","title":"ppo.py"},{"location":"rl-algorithms/ppo/#usage","text":"poetry install python cleanrl/ppo.py --help python cleanrl/ppo.py --env-id CartPole-v1","title":"Usage"},{"location":"rl-algorithms/ppo/#implementation-details","text":"ppo.py includes the 11 core implementation details: Vectorized architecture ( common/cmd_util.py#L22 ) Orthogonal Initialization of Weights and Constant Initialization of biases ( a2c/utils.py#L58) ) The Adam Optimizer's Epsilon Parameter ( ppo2/model.py#L100 ) Adam Learning Rate Annealing ( ppo2/ppo2.py#L133-L135 ) Generalized Advantage Estimation ( ppo2/runner.py#L56-L65 ) Mini-batch Updates ( ppo2/ppo2.py#L157-L166 ) Normalization of Advantages ( ppo2/model.py#L139 ) Clipped surrogate objective ( ppo2/model.py#L81-L86 ) Value Function Loss Clipping ( ppo2/model.py#L68-L75 ) Overall Loss and Entropy Bonus ( ppo2/model.py#L91 ) Global Gradient Clipping ( ppo2/model.py#L102-L108 )","title":"Implementation details"},{"location":"rl-algorithms/ppo/#experiment-results","text":"PR vwxyzjn/cleanrl#120 tracks our effort to conduct experiments, and the reprodudction instructions can be found at vwxyzjn/cleanrl/benchmark/ppo . Below are the average episodic returns for ppo.py . To ensure the quality of the implementation, we compared the results against openai/baselies ' PPO. Environment ppo.py openai/baselies ' PPO CartPole-v1 488.75 \u00b1 18.40 497.54 \u00b1 4.02 Acrobot-v1 -82.48 \u00b1 5.93 -81.82 \u00b1 5.58 MountainCar-v0 -200.00 \u00b1 0.00 -200.00 \u00b1 0.00 Learning curves: Tracked experiments and game play videos:","title":"Experiment results"},{"location":"rl-algorithms/ppo/#video-tutorial","text":"If you'd like to learn ppo.py in-depth, consider checking out the following video tutorial:","title":"Video tutorial"},{"location":"rl-algorithms/ppo/#ppo_ataripy","text":"The ppo_atari.py has the following features: For playing Atari games. It uses convolutional layers and common atari-based pre-processing techniques. Works with the Atari's pixel Box observation space of shape (210, 160, 3) Works with the Discerete action space Includes the 9 Atari-specific implementation details as shown in the following video tutorial","title":"ppo_atari.py"},{"location":"rl-algorithms/ppo/#ppo_continuous_actionpy","text":"The ppo_continuous_action.py has the following features: For continuous action space. Also implemented Mujoco-specific code-level optimizations Works with the Box observation space of low-level features Works with the Box (continuous) action space Includes the 8 implementation details for as shown in the following video tutorial (need fixing)","title":"ppo_continuous_action.py"}]}